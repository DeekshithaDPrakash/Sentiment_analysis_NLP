{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         sentiment          id                          date     query  \\\n",
      "0                0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
      "1                0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
      "2                0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
      "3                0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "4                0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "...            ...         ...                           ...       ...   \n",
      "1599995          4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
      "1599996          4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
      "1599997          4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
      "1599998          4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
      "1599999          4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
      "\n",
      "                username                                               text  \n",
      "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
      "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
      "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
      "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
      "...                  ...                                                ...  \n",
      "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
      "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
      "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
      "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
      "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
      "\n",
      "[1600000 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check colum attributes\n",
    "\n",
    "# Load the Sentiment140 dataset\n",
    "df = pd.read_csv('twitter_sentiment.csv', names=[\"sentiment\", \"id\", \"date\", \"query\", \"username\", \"text\"], encoding = \"ISO-8859-1\")\n",
    "\n",
    "X = df['text'].values\n",
    "y = df['sentiment'].values\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 4 4 4]\n",
      "[\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\"\n",
      " \"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\"\n",
      " '@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds'\n",
      " ... 'Are you ready for your MoJo Makeover? Ask me for details '\n",
      " 'Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur '\n",
      " 'happy #charitytuesday @theNSPCC @SparksCharity @SpeakingUpH4H ']\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26964\\4119128975.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['sentiment'] = data['sentiment'].replace({0: 'negative', 4: 'positive'})\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26964\\4119128975.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['text'] = data['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x.lower()))\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26964\\4119128975.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['text'] = data['text'].apply(lambda x: word_tokenize(x))\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_26964\\4119128975.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['text'] = data['text'].apply(lambda x: [word for word in x if word not in stop_words])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Keep only the relevant columns (text and sentiment)\n",
    "data = df[['text', 'sentiment']]\n",
    "\n",
    "# Convert sentiment column to 0 (negative) and 1 (positive)\n",
    "data['sentiment'] = data['sentiment'].replace({0: 'negative', 4: 'positive'})\n",
    "\n",
    "# Remove special characters and lowercase the text\n",
    "data['text'] = data['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x.lower()))\n",
    "\n",
    "# Tokenize the text\n",
    "data['text'] = data['text'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data['text'] = data['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "data\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data = data.sample(frac=0.8, random_state=200)\n",
    "test_data = data.drop(train_data.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[switchfoot, httptwitpiccom2y1zl, awww, thats,...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[upset, cant, update, facebook, texting, might...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[kenichan, dived, many, times, ball, managed, ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[whole, body, feels, itchy, like, fire]</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[nationwideclass, behaving, im, mad, cant, see]</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>[woke, school, best, feeling, ever]</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>[thewdbcom, cool, hear, old, walt, interviews,...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>[ready, mojo, makeover, ask, details]</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>[happy, 38th, birthday, boo, alll, time, tupac...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>[happy, charitytuesday, thenspcc, sparkscharit...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text sentiment\n",
       "0        [switchfoot, httptwitpiccom2y1zl, awww, thats,...  negative\n",
       "1        [upset, cant, update, facebook, texting, might...  negative\n",
       "2        [kenichan, dived, many, times, ball, managed, ...  negative\n",
       "3                  [whole, body, feels, itchy, like, fire]  negative\n",
       "4          [nationwideclass, behaving, im, mad, cant, see]  negative\n",
       "...                                                    ...       ...\n",
       "1599995                [woke, school, best, feeling, ever]  positive\n",
       "1599996  [thewdbcom, cool, hear, old, walt, interviews,...  positive\n",
       "1599997              [ready, mojo, makeover, ask, details]  positive\n",
       "1599998  [happy, 38th, birthday, boo, alll, time, tupac...  positive\n",
       "1599999  [happy, charitytuesday, thenspcc, sparkscharit...  positive\n",
       "\n",
       "[1600000 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'don', 'hadn', 'while', 'can', 'there', 'same', 'he', 'has', 'isn', 'o', 'll', 'we', \"you've\", 'herself', 'doesn', 'myself', 'it', 'a', 'who', 'which', 'are', 'm', 'against', 'd', \"hasn't\", 'between', 'being', 'once', \"shan't\", 'they', 'what', 'didn', 'wouldn', 'i', 'into', 'mightn', 'you', 'by', 't', 'hasn', 'couldn', 'theirs', \"mightn't\", \"you'll\", 'from', 'too', 'be', 'for', 'before', 'just', 'on', 'yourself', 'during', 'had', 'both', 'yours', 'after', 'why', 'weren', 'no', 'did', 'over', 'all', 'themselves', 'she', 'me', 'have', 'will', 'mustn', 'off', 'but', 'when', 'now', 'its', \"aren't\", 'other', 'with', 'out', 'some', 'any', 'y', 'their', 'if', 'in', 'here', 'this', \"should've\", \"hadn't\", 'so', 'each', \"won't\", \"wouldn't\", 'about', 'not', 'only', 'through', 'our', 'own', 'doing', 'an', 'that', 'himself', 'haven', \"mustn't\", \"shouldn't\", 'hers', \"couldn't\", 'ma', 'your', 'his', 'or', \"weren't\", 'very', 'whom', 'of', \"you'd\", \"she's\", 'shan', 'am', 'is', 'because', 'itself', 'down', \"doesn't\", 'does', 'then', \"don't\", 'below', 'at', 'ours', 'few', 'aren', \"you're\", 'further', 'nor', 'most', \"needn't\", 'under', 'as', 'having', 'do', 'up', \"isn't\", 'such', 'ourselves', 'those', 'the', 'needn', 'these', 'yourselves', 'her', 've', 'them', 'him', 's', \"it's\", 'was', 'where', 'than', 'to', 'how', 'should', 'ain', 'more', 'won', 'and', 'until', 'again', \"wasn't\", 're', \"didn't\", 'been', \"that'll\", 'wasn', \"haven't\", 'above', 'my', 'shouldn', 'were'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "train_data = data.sample(frac=0.8, random_state=200)\n",
    "test_data = data.drop(train_data.index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The next step after preprocessing the data is to convert the tokenized text into numerical representations that can be used as input for a machine learning model. One way to do this is to use the bag of words representation. In this representation, each document is represented as a vector of word frequencies, where each word in the vocabulary is a dimension.\n",
    "\n",
    "######  The CountVectorizer class is used to convert the tokenized text into bag of words representation.\n",
    "\n",
    "###### The fit_transform method is used to fit the count vectorizer to the training data and transform it into a numerical representation\n",
    "\n",
    "###### The transform method is used to transform the test data into a numerical representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Convert the tokenized text into bag of words representation\n",
    "count_vectorizer = CountVectorizer()\n",
    "train_text = [' '.join(text) for text in train_data['text'].values] #removes commas. Ex: [i, love, you] ---> [i love you]\n",
    "test_text = [' '.join(text) for text in test_data['text'].values]\n",
    "train_text = count_vectorizer.fit_transform(train_text)\n",
    "test_text = count_vectorizer.transform(test_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\pt117\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logistic regression training\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train a logistic regression classifier\n",
    "sentiment_classifier = LogisticRegression(random_state=0)\n",
    "sentiment_classifier.fit(train_text, train_data['sentiment'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "# Predict sentiments for the test data\n",
    "predicted_sentiments = sentiment_classifier.predict(test_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78460625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_data['sentiment'].values, predicted_sentiments)\n",
    "print('Accuracy:', accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt117",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c545f025833ef80473ae121570365daf71fd7d3ea432671ee0f341d5ef01b18e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
